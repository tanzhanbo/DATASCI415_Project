{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.loadtxt(\"dataset/train/X_train.txt\")\n",
    "y_train = np.loadtxt(\"dataset/train/y_train.txt\", dtype=int) - 1 # Adjust labels to start at 0\n",
    "X_test = np.loadtxt(\"dataset/test/X_test.txt\")\n",
    "y_test = np.loadtxt(\"dataset/test/y_test.txt\", dtype=int) - 1 # Adjust labels to start at 0\n",
    "train_set = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "test_set = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss():\n",
    "    return\n",
    "\n",
    "def train(train_set, model, criterion, epochs=10, lr=0.01, reg_lambda=0.001, print_epoch=True):\n",
    "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            if criterion == hinge_loss:\n",
    "                loss = criterion(outputs, y, model, reg_lambda)\n",
    "            else:\n",
    "                loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if print_epoch:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "    return total_loss\n",
    "\n",
    "def test(test_set, model):\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    y_test = test_set.tensors[1].numpy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            outputs = model(X)\n",
    "            preds = torch.argmax(outputs, axis=1).numpy()\n",
    "            all_preds.extend(preds)\n",
    "    print(\"Accuracy Score:\", accuracy_score(y_test, all_preds))\n",
    "    print(classification_report(y_test, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.01, Loss: 44.576213389635086\n",
      "Learning rate: 0.0325, Loss: 22.39434902369976\n",
      "Learning rate: 0.05500000000000001, Loss: 19.76302805542946\n",
      "Learning rate: 0.0775, Loss: 18.706474594771862\n",
      "Learning rate: 0.1, Loss: 23.30612576752901\n",
      "Best Learning Rate: 0.0775, Best Loss: 18.7065\n",
      "Epoch 1/10, Loss: 15.7290\n",
      "Epoch 2/10, Loss: 15.4013\n",
      "Epoch 3/10, Loss: 14.1330\n",
      "Epoch 4/10, Loss: 14.5722\n",
      "Epoch 5/10, Loss: 13.6389\n",
      "Epoch 6/10, Loss: 11.9251\n",
      "Epoch 7/10, Loss: 13.7429\n",
      "Epoch 8/10, Loss: 12.1428\n",
      "Epoch 9/10, Loss: 12.7130\n",
      "Epoch 10/10, Loss: 12.5556\n",
      "Accuracy Score: 0.8900576857821514\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       496\n",
      "           1       0.92      0.96      0.94       471\n",
      "           2       0.97      0.93      0.95       420\n",
      "           3       1.00      0.48      0.65       491\n",
      "           4       0.68      1.00      0.81       532\n",
      "           5       1.00      1.00      1.00       537\n",
      "\n",
      "    accuracy                           0.89      2947\n",
      "   macro avg       0.92      0.89      0.88      2947\n",
      "weighted avg       0.92      0.89      0.88      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class FCNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=6):\n",
    "        super(FCNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "# model = FCNet(input_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# train(train_set, model, criterion)\n",
    "# test(test_set, model)\n",
    "\n",
    "lrs = np.linspace(0.01, 0.1, 5)\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_lr = None\n",
    "\n",
    "for lr in lrs:  \n",
    "    model = FCNet(input_size)\n",
    "    train_loss = train(train_set, model, criterion, lr=lr, print_epoch=False)\n",
    "    print(f\"Learning rate: {lr:.4f}, Loss: {train_loss:.4f}\")\n",
    "        \n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        best_lr = lr\n",
    "\n",
    "print(f\"Best Learning Rate: {best_lr:.4f}, Best Loss: {best_loss:.4f}\")\n",
    "\n",
    "fc_model = FCNet(input_size)\n",
    "train(train_set, model, criterion, lr=best_lr)\n",
    "test(test_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model Accuracy: 0.9504580929759077\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       496\n",
      "           1       0.93      0.96      0.94       471\n",
      "           2       0.99      0.91      0.95       420\n",
      "           3       0.94      0.89      0.91       491\n",
      "           4       0.91      0.95      0.93       532\n",
      "           5       1.00      1.00      1.00       537\n",
      "\n",
      "    accuracy                           0.95      2947\n",
      "   macro avg       0.95      0.95      0.95      2947\n",
      "weighted avg       0.95      0.95      0.95      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_svm_model(X_train, y_train, X_test, y_test):\n",
    "    svm_model = SVC(kernel='rbf', C=1.0)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    svm_preds = svm_model.predict(X_test)\n",
    "    \n",
    "    print(\"SVM Model Accuracy:\", accuracy_score(y_test, svm_preds))\n",
    "    print(classification_report(y_test, svm_preds))\n",
    "    return svm_model\n",
    "\n",
    "svm_model = train_svm_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001, Lambda: 0.0001, Loss: 4535.143295288086\n",
      "Learning rate: 0.0001, Lambda: 0.000325, Loss: 4536.546081542969\n",
      "Learning rate: 0.0001, Lambda: 0.00055, Loss: 4508.832475662231\n",
      "Learning rate: 0.0001, Lambda: 0.0007750000000000001, Loss: 4468.438283920288\n",
      "Learning rate: 0.0001, Lambda: 0.001, Loss: 4527.887773513794\n",
      "Learning rate: 0.000325, Lambda: 0.0001, Loss: 2981.1092824935913\n",
      "Learning rate: 0.000325, Lambda: 0.000325, Loss: 3047.769739151001\n",
      "Learning rate: 0.000325, Lambda: 0.00055, Loss: 3004.663818359375\n",
      "Learning rate: 0.000325, Lambda: 0.0007750000000000001, Loss: 2992.687195777893\n",
      "Learning rate: 0.000325, Lambda: 0.001, Loss: 3035.6607751846313\n",
      "Learning rate: 0.00055, Lambda: 0.0001, Loss: 2935.7483010292053\n",
      "Learning rate: 0.00055, Lambda: 0.000325, Loss: 2847.57759475708\n",
      "Learning rate: 0.00055, Lambda: 0.00055, Loss: 2948.243597507477\n",
      "Learning rate: 0.00055, Lambda: 0.0007750000000000001, Loss: 2937.0466623306274\n",
      "Learning rate: 0.00055, Lambda: 0.001, Loss: 2931.2954902648926\n",
      "Learning rate: 0.0007750000000000001, Lambda: 0.0001, Loss: 2875.0952005386353\n",
      "Learning rate: 0.0007750000000000001, Lambda: 0.000325, Loss: 3043.580379486084\n",
      "Learning rate: 0.0007750000000000001, Lambda: 0.00055, Loss: 2766.5370931625366\n",
      "Learning rate: 0.0007750000000000001, Lambda: 0.0007750000000000001, Loss: 2791.7819395065308\n",
      "Learning rate: 0.0007750000000000001, Lambda: 0.001, Loss: 2726.721024990082\n",
      "Learning rate: 0.001, Lambda: 0.0001, Loss: 3067.684051990509\n",
      "Learning rate: 0.001, Lambda: 0.000325, Loss: 2950.065935611725\n",
      "Learning rate: 0.001, Lambda: 0.00055, Loss: 3102.306962490082\n",
      "Learning rate: 0.001, Lambda: 0.0007750000000000001, Loss: 2731.735472202301\n",
      "Learning rate: 0.001, Lambda: 0.001, Loss: 2882.158893585205\n",
      "Best Learning Rate: 0.0007750000000000001, Best Lambda: 0.001, Best Loss: 2726.7210\n",
      "Epoch 1/10, Loss: 16496.4834\n",
      "Epoch 2/10, Loss: 7533.0281\n",
      "Epoch 3/10, Loss: 5800.2445\n",
      "Epoch 4/10, Loss: 4475.7622\n",
      "Epoch 5/10, Loss: 4387.6788\n",
      "Epoch 6/10, Loss: 3859.4390\n",
      "Epoch 7/10, Loss: 3579.0706\n",
      "Epoch 8/10, Loss: 3261.4682\n",
      "Epoch 9/10, Loss: 3177.1342\n",
      "Epoch 10/10, Loss: 2826.8905\n",
      "Accuracy Score: 0.9416355615880556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       496\n",
      "           1       0.98      0.92      0.95       471\n",
      "           2       0.93      0.99      0.96       420\n",
      "           3       0.92      0.84      0.88       491\n",
      "           4       0.87      0.93      0.90       532\n",
      "           5       0.99      1.00      1.00       537\n",
      "\n",
      "    accuracy                           0.94      2947\n",
      "   macro avg       0.94      0.94      0.94      2947\n",
      "weighted avg       0.94      0.94      0.94      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class SVM(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=6):\n",
    "        super(SVM, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def hinge_loss(output, y, model, reg_lambda):\n",
    "    y_one_hot = torch.zeros_like(output)\n",
    "    y_one_hot.scatter_(1, y.view(-1, 1), 1) # Convert labels 0-5 to one-hot encoding\n",
    "    y_one_hot = 2 * y_one_hot - 1  # Convert to {-1, 1} for each class\n",
    "\n",
    "    loss = torch.sum(torch.clamp(1 - y_one_hot * output, min=0))\n",
    "    reg_term = reg_lambda * torch.sum(model.linear.weight ** 2)\n",
    "    return loss + reg_term\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "# model = SVM(input_size)\n",
    "criterion = hinge_loss\n",
    "\n",
    "# train(train_set, svm_model, criterion)\n",
    "# test(test_set, svm_model)\n",
    "\n",
    "# Hyperparameter search\n",
    "lrs = np.linspace(0.0001, 0.001, 5)\n",
    "lambdas = np.linspace(0.0001, 0.001, 5)\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_lr = None\n",
    "best_lambda = None\n",
    "\n",
    "for lr in lrs:\n",
    "    for reg_lambda in lambdas:   \n",
    "        model = SVM(input_size)\n",
    "        train_loss = train(train_set, model, criterion, lr=lr, reg_lambda=reg_lambda, print_epoch=False)\n",
    "        print(f\"Learning rate: {lr:.4f}, Lambda: {reg_lambda:.4f}, Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            best_lr = lr\n",
    "            best_lambda = reg_lambda\n",
    "\n",
    "print(f\"Best Learning Rate: {best_lr:.4f}, Best Lambda: {best_lambda:.4f}, Best Loss: {best_loss:.4f}\")\n",
    "\n",
    "model = SVM(input_size)\n",
    "train(train_set, model, criterion, lr=best_lr, reg_lambda=best_lambda)\n",
    "test(test_set, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9257\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93       496\n",
      "           1       0.90      0.89      0.89       471\n",
      "           2       0.97      0.87      0.91       420\n",
      "           3       0.91      0.89      0.90       491\n",
      "           4       0.90      0.92      0.91       532\n",
      "           5       1.00      1.00      1.00       537\n",
      "\n",
      "    accuracy                           0.93      2947\n",
      "   macro avg       0.93      0.92      0.92      2947\n",
      "weighted avg       0.93      0.93      0.93      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,  # Number of trees\n",
    "    max_depth=None,    # Maximum depth of trees (default)\n",
    "    random_state=42    # Set for reproducibility\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
